{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uniform Convergence**\n",
    "\n",
    "Let's now consider the problem of making electoral predictions in the fictitious country of Elbania (or, wherever, really).\n",
    "\n",
    "Let there be $n$ states, and let $m$ voters be drawn IID from each state. Let the actual fraction of voters in state $i$ that voted democrat be $\\phi_i$. Also let $X_{ij}$, $(1 \\leq i \\leq n, 1 \\leq j \\leq m)$ be a binary random variable indicating whether the $j$-th randomly chosen voter from state $i$ voted democrat.\n",
    "\n",
    "Assume people don't lie about their votes, so $X_{ij}$ is drawn from a Bernoulli distribution, and assume all $X_{ij}$ are mutually independent.\n",
    "\n",
    "After the survey, our estimate for the fraction of democratic voters in a state is\n",
    "\n",
    "$\\hat{\\phi}_i = \\frac{1}{m} \\sum_j X_{ij}$,\n",
    "\n",
    "and we define $Z_i = 1\\left\\{\\left|\\hat{\\phi}_i-\\phi_i\\right| \\gt \\gamma\\right\\}$ as a binary variable that indicates whether or not the predicition was highly accurate for state $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Let $\\psi_i$ be the probability of $Z_i = 1$. Use Hoeffding's inequality to upper bound $\\psi_i$.\n",
    "\n",
    "Note\n",
    "\n",
    "$\\psi_i = P\\left(\\left|\\hat{\\phi}_i-\\phi_i\\right| \\gt \\gamma \\right)$.\n",
    "\n",
    "Hoeffding's inequality tells us directly that\n",
    "\n",
    "$\\psi_i \\leq 2\\text{exp}\\left(-2m\\gamma^2\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Let $V_i$, $W_i$, $(1 \\leq i \\leq k)$ be Bernoulli random variables, and suppose $E[V_i] = P(V_i = 1) \\leq P(W_i = 1) = E[W_i]$ for all $i$, with all $V_i$ and $W_i$ mutually independent. Prove that, for any value of $t$, the following holds:\n",
    "\n",
    "$P\\left(\\sum_k V_i \\gt t\\right) \\leq P\\left(\\sum_k W_i \\gt t\\right)$.\n",
    "\n",
    "Let's prove this by induction.\n",
    "\n",
    "*Base case,* $k = 1$:\n",
    "\n",
    "Consider $t \\lt 0$. Then, since $V_1$ is a binary variable (either 0 or 1), $P\\left(V_1 \\gt t\\right) = 1$. Ditto for $P\\left(W_1 \\gt t\\right)$. Thus, $P\\left(V_1 \\gt t\\right) = P\\left(W_1 \\gt t\\right)$ for $t \\lt 0$. Likewise, if $t \\geq 1$, $P\\left(V_1 \\gt t\\right) = P\\left(W_1 \\gt t\\right) = 0$.\n",
    "\n",
    "Now, consider $0 \\geq t \\lt 1$. Notice that $P\\left(V_1 \\gt t\\right) = P(V_1 = 1)$ and $P\\left(W_1 \\gt t\\right) = P(W_1 = 1)$. Since we know $P(V_1 = 1) \\leq P(W_1 = 1)$ by assumption, the formula holds, and we've proven the base case.\n",
    "\n",
    "*Induction,* $k = n \\rightarrow k = n+1$\n",
    "\n",
    "Now, assume the formula holds up to $k = n$, and consider $k = n+1$. We have\n",
    "\n",
    "$P\\left(\\sum_{k=1}^{n+1} V_i \\gt t\\right) = P\\left(V_{n+1}+\\sum_{k=1}^{n} V_i \\gt t\\right) = P\\left(\\sum_{k=1}^{n} V_i \\gt t\\right) + P(V_{n+1}=1)P\\left(\\sum_{k=1}^{n} V_i \\gt t-1\\right)$,\n",
    "\n",
    "and similarly for $W_i$. Now, since we have assumed the formula holds true up to $k = n$, we can immediately write\n",
    "\n",
    "$P\\left(\\sum_{k=1}^{n+1} V_i \\gt t\\right) \\leq P\\left(\\sum_{k=1}^{n} W_i \\gt t\\right) + P(V_{n+1}=1)P\\left(\\sum_{k=1}^{n} W_i \\gt t-1\\right)$.\n",
    "\n",
    "Finally, using the assumption that $P(V_i = 1) \\leq P(W_i = 1)$ for all $i$, we get\n",
    "\n",
    "$P\\left(\\sum_{k=1}^{n+1} V_i \\gt t\\right) \\leq P\\left(\\sum_{k=1}^{n} W_i \\gt t\\right) + P(W_{n+1}=1)P\\left(\\sum_{k=1}^{n} W_i \\gt t-1\\right) = P\\left(\\sum_{k=1}^{n+1} W_i \\gt t\\right)$,\n",
    "\n",
    "which proves the formula for $k = n+1$. Thus, by induction the formula holds true for all $k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** The fraction $P_n$ of states on which our predictions are highly inaccurate is given by $\\bar{Z} = \\frac{1}{n}\\sum_i Z_i$. Prove a reasonable closed form upper bound on the probability $P\\left(\\bar{Z} \\gt \\tau\\right)$ of being highly inaccurate on more than a fraction $\\tau$ of the states.\n",
    "\n",
    "Let $Y_i$ be some new random binary variable, with $E[Y_i] = P(Y_i = 1) = 2\\text{exp}\\left(-2m\\gamma^2\\right)$. Then, from our result in **(a)** we know $P(Z_i = 1) \\leq P(Y_i = 1)$. Thus, from our result in **(b)** it immediately follows that\n",
    "\n",
    "$P\\left(\\bar{Z} \\gt \\tau\\right) \\leq P\\left(\\frac{1}{n}\\sum_i Y_i \\gt \\tau\\right)$.\n",
    "\n",
    "Now, let $P(Y_i = 1) = \\mu$, and note that\n",
    "\n",
    "$P\\left(\\frac{1}{n}\\sum_i Y_i \\gt \\tau\\right) = P\\left(\\frac{1}{n}\\sum_i Y_i-\\mu \\gt \\tau-\\mu\\right) \\leq P\\left(\\left|\\frac{1}{n}\\sum_i Y_i-\\mu\\right| \\gt \\tau-\\mu\\right)$.\n",
    "\n",
    "Notice that the right hand side can be written as $P\\left(\\left|\\hat{\\mu}-\\mu\\right| \\gt \\tau-\\mu\\right)$, and provided that $\\tau-\\mu \\gt 0$ we can apply Hoeffding's inequality once again to find that\n",
    "\n",
    "$P\\left(\\frac{1}{n}\\sum_i Y_i \\gt \\tau\\right) \\leq 2\\text{exp}\\left(-2n(\\tau-\\mu)^2\\right)$.\n",
    "\n",
    "Thus, an upper bound is\n",
    "\n",
    "$P\\left(\\bar{Z} \\gt \\tau\\right) \\leq 2\\text{exp}\\left(-2n(\\tau-\\mu)^2\\right)$, $\\mu = 2\\text{exp}\\left(-2m\\gamma^2\\right)$,\n",
    "\n",
    "subject to the condition $\\tau-\\mu\\gt 0 \\rightarrow m \\gt \\frac{\\text{log}(2/\\tau)}{2\\gamma^2}$. Now, note that this bound goes to 0 as $n \\rightarrow \\infty$, but as $m \\rightarrow \\infty$ for fixed $\\tau$, $n$, it does *not* go to zero. This means it is insufficient, because intuitively we know that as we sample more and more we should converge to the correct predicitions for all states.\n",
    "\n",
    "So, consider again\n",
    "\n",
    "$P\\left(\\bar{Z} \\gt \\tau\\right) \\leq P\\left(\\frac{1}{n}\\sum_i Y_i \\gt \\tau\\right) = P\\left(\\sum_i Y_i \\gt n\\tau\\right)$.\n",
    "\n",
    "Let $k$ be the smallest integer such that $k \\gt n\\tau$, and we find\n",
    "\n",
    "$P\\left(\\sum_i Y_i \\gt n\\tau\\right) = \\sum_{j=k}^n P\\left(\\sum_i Y_i = j\\right)$.\n",
    "\n",
    "The right hand side can be explicitly evaluated since $P\\left(\\sum_i Y_i = j\\right) = {n \\choose j} \\mu^j (1-\\mu)^{1-j}$, so that \n",
    "\n",
    "$P\\left(\\sum_i Y_i \\gt n\\tau\\right) = \\sum_{j=k}^n {n \\choose j} \\mu^j (1-\\mu)^{1-j} \\leq \\sum_{j=k}^n {n \\choose j} \\mu^j$.\n",
    "\n",
    "Thus, another upper bound is\n",
    "\n",
    "$P\\left(\\bar{Z} \\gt \\tau\\right) \\leq \\sum_{j=k}^n {n \\choose j} \\mu^j$.\n",
    "\n",
    "This does decrease to 0 as $m \\rightarrow \\infty$ since $\\mu \\rightarrow 0$. So, our final upper bound can be the minimum of these two bounds for any set of $m$, $n$, $\\tau$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More VC Dimension**\n",
    "\n",
    "Let the domain of the inputs for a learning problem be $\\mathcal{X} = \\mathbb{R}$. Consider using hypotheses of the following form:\n",
    "\n",
    "$h_\\theta(x) = 1\\left\\{\\theta_0 + \\theta_1 x + \\theta_2 x^2 + ... + \\theta_d x^d \\geq 0\\right\\}$,\n",
    "\n",
    "and let $\\mathcal{H} = \\left\\{h_\\theta : \\theta \\in \\mathbb{R}^{d+1}\\right\\}$ be the corresponding hypothesis class. What is the VC dimension of $\\mathcal{H}$?\n",
    "\n",
    "Recall that any $d$-degree polynomial has at most $d$ roots. Thus, we can define $f(x) = \\pm \\Pi_{i=1}^d (x-r_i)$, where all $r_i$ are distinct, and $f \\in \\mathcal{H}$. Note that if, for any two points $x_j$ and $x_k$ and any root $r_i$, we have $x_j \\lt r_i \\lt x_k$, then $x_j$ and $x_k$ must have different labels, otherwise they have the same label.\n",
    "\n",
    "Now, consider a set of $d+1$ distinct points $\\left\\{x_0,x_1,...,x_d\\right\\}$, and a set of labels $\\left\\{y_0,y_1,...,y_d\\right\\}$. We will construct a set of roots $\\left\\{r_1,r_2,...r_d\\right\\}$ which defines a hypothesis $h$ as described above such that all points are correctly labeled. Thus, we will show that $\\mathcal{H}$ shatters this set, and that $VC(\\mathcal{H}) \\geq d+1$.\n",
    "\n",
    "Find the first point $x_j$ such that $y_j \\neq y_0$, and set $r_1 = (x_j+x_{j-1})/2$. Then, find the first point $x_k$ such that $y_k \\neq y_j$ and set $r_2 = (x_k+x_{k-1})/2$. Continue in this fashion until we've assigned all roots or until we have run out of points $(x_i,y_i)$. In the latter case, set all remaining roots to be greater than $x_d$. Note that even in the case that all $d+1$ labels are distinct we will only need $d$ roots placed at the midpoints, so we can always assign roots in this way. This procedure will ensure that for all $x_j,x_k$, $h(x_j) \\neq h(x_k)$ iff $y_j \\neq y_k$ due to the logic we set out above.\n",
    "\n",
    "Now, notice that $x_0$ is less than all roots $r_i$, so that $h(x_0) = \\pm (-1)^d$. If $y_0 = 1$, define $h(x) = \\Pi_{i=1}^d (x-r_i)$ if $d$ is even and $h(x) = -\\Pi_{i=1}^d (x-r_i)$ if $d$ is odd. If $y_0 = 0$, do the opposite. This ensures that $y_0$ is labeled correctly, and since we also preserve the distinctness of labels (as described above) this ensures that $h(x)$ correctly labels all $d+1$ points, thus $VC(\\mathcal{H}) \\geq d+1$.\n",
    "\n",
    "We still need to prove that $\\mathcal{H}$ cannot shatter a $d+2$ size set, which then proves that $VC(\\mathcal{H}) = d+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAP Estimates and Weight Decay**\n",
    "\n",
    "Consider using a logistic regression model $h_\\theta(x) = g(\\theta^T x)$, where $g$ is the sigmoid function, and let a training set $\\left\\{(x^{(i)},y^{(i)}); i = 1,...,m\\right\\}$ be given as usual. The maximum likelihood estimate of the parameters $\\theta$ is given by\n",
    "\n",
    "$\\theta_{ML} = \\text{arg max}_\\theta \\Pi_{i=1}^m p\\left(y^{(i)} | x^{(i)} ;\\theta\\right)$.\n",
    "\n",
    "If we wanted to regularize logistic regression, then we might put a Bayesian prior on the parameters. Suppose we chose the prior $\\theta \\sim N(0,\\tau^2 I)$ ($\\tau \\gt 0$, and $I$ is the $n+1$-by-$n+1$ identity matrix), and then found the MAP estimate of $\\theta$ as:\n",
    "\n",
    "$\\theta_{MAP} = \\text{arg max}_\\theta p(\\theta) \\Pi_{i=1}^m p\\left(y^{(i)} | x^{(i)},\\theta\\right)$.\n",
    "\n",
    "Prove that\n",
    "\n",
    "$||\\theta_{MAP}||_2 \\leq ||\\theta_{ML}||_2$.\n",
    "\n",
    "Assume the statement is not true. That is, assume $||\\theta_{MAP}||_2 \\gt ||\\theta_{ML}||_2$. Now, notice that\n",
    "\n",
    "$p(\\theta) \\sim \\text{exp}\\left(-||\\theta||_2/\\tau^2\\right)$,\n",
    "\n",
    "so that in this case $p\\left(\\theta_{MAP}\\right) \\lt p\\left(\\theta_{ML}\\right)$. Notice also that\n",
    "\n",
    "$\\Pi_{i=1}^m p\\left(y^{(i)} | x^{(i)},\\theta_{MAP}\\right) \\leq \\Pi_{i=1}^m p\\left(y^{(i)} | x^{(i)},\\theta_{ML}\\right)$,\n",
    "\n",
    "since $\\theta_{ML}$ is by definition that $\\theta$ which maximizes this quantity. In this case, we find that\n",
    "\n",
    "$p\\left(\\theta_{MAP}\\right) \\Pi_{i=1}^m p\\left(y^{(i)} | x^{(i)},\\theta_{MAP}\\right) \\lt p\\left(\\theta_{ML}\\right) \\Pi_{i=1}^m p\\left(y^{(i)} | x^{(i)},\\theta_{ML}\\right)$.\n",
    "\n",
    "which is a contradiction since $\\theta_{MAP}$ is defined as the $\\theta$ which maximizes this quantity. Thus, our initial assumption must be wrong and in fact\n",
    "\n",
    "$||\\theta_{MAP}||_2 \\leq ||\\theta_{ML}||_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
