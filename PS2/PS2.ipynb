{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import randint as randint\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constructing kernels**\n",
    "\n",
    "Let $K_1$, $K_2$ be (Mercer) kernels over $\\mathbb{R}^n\\times \\mathbb{R}^n$, let $a \\in \\mathbb{R}^+$ be a positive real number, let $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ be a real-valued function, let $\\phi : \\mathbb{R}^n \\rightarrow \\mathbb{R}^d$ be a function mapping from $\\mathbb{R}^n$ to $\\mathbb{R}^d$, let $K_3$ be a kernel over $\\mathbb{R}^d \\times \\mathbb{R}^d$, and let $p(x)$ be a polynomial over $x$ with *positive* coefficients. We know want to know which of the following functions are also kernels. That is, which are symmetric, positive semi-definite matrices for some finite set $\\{x^{(1)},...,x^{(m)}\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) $K(x,z) = K_1(x,z) + K_2(x,z)$\n",
    "\n",
    "This is clearly a kernel. Using the fact that $K_1$ and $K_2$ are kernels, and therefore symmetric, we find $K_{ji} = K_{1,ji} + K_{2,ji} = K_{1,ij} + K_{2,ij} = K_{ij}$, so that $K$ is symmetric. Similarly, the fact that $z^T Kz \\geq 0$, $\\forall z \\in \\mathbb{R}^n$ follows from linearity and the positive semi-definite nature of both $K_1$ and $K_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) $K(x,z) = K_1(x,z) - K_2(x,z)$\n",
    "\n",
    "This is not a kernel. Let $K_2 = b K_1$, $b >1$ and $b \\in \\mathbb{R}^+$. Then, notice that $z^T Kz = (1-b) (z^T K_1z) \\leq 0$ since $K_1$ is positive semi-definite. So, in this case $K$ is negative semi-definite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) $K(x,z) = aK_1(x,z)$\n",
    "\n",
    "This is a kernel. Transposition and matrix multiplication commute with scalar multiplication, and $K$ is therefore symmetric following directly from the symmetric nature of $K_1$. Since $a$ is a *positive* real, and $K_1$ is positive semi-definite, the positive semi-definite nature of $K$ follows in a similar fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) $K(x,z) = -aK_1(x,z)$\n",
    "\n",
    "This is *not* a kernel. In particular, notice that $z^T Kz = -az^T K_1 z \\leq 0$ since $K_1$ is positive semi-definite and $a$ is a positive real. This means that $K$ is a negative semi-definite matrix and cannot be a kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) $K(x,z) = K_1(x,z)K_2(x,z)$\n",
    "\n",
    "This is a kernel. Recall that, as kernels, we can write $K_1(x,z) = \\phi_1^T(x) \\phi_1(z)$ and $K_2 = \\phi_2^T(x) \\phi_2(z)$. Thus, $K(x,z) = \\phi_1^T(x) \\phi_1(z) \\phi_2^T(x) \\phi_2(z) = \\sum_{i,j} \\phi_{1,i}(x) \\phi_{1,i}(z) \\phi_{2,j}(x) \\phi_{2,j}(z)$. If we define $\\Phi = \\phi_1 \\otimes \\phi_2 \\rightarrow \\Phi_{i,j} = \\phi_{1,i} \\phi_{2,j}$, we then have $K(x,z) = \\Phi(x)^T \\Phi$, so $K$ has the appropriate form to be a kernel as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) $K(x,z) = f(x)f(z)$.\n",
    "\n",
    "This is a kernel. Since $f$ is a scalar, we can just define $\\phi = f$ and $K$ trivially has the form $K(x,z) = \\phi^T(x) \\phi(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kernelizing the perceptron**\n",
    "\n",
    "The original update rule we want to modify is $\\theta := \\theta + \\alpha y^{(i)} x^{(i)}$ if $y^{(i)}h\\left(x^{(i)}\\right) < 0$, and $\\theta := \\theta$ otherwise. Working in the high-dimensional space, we now use the modified update rule $\\theta := \\theta + \n",
    "\\alpha \\left(y^{(i)}-h\\left(\\theta^T \\phi\\left(x^{(i)}\\right)\\right)\\right)\\phi\\left(x^{(i)}\\right)$. Note that this follows the same spirit as the original update rule: if $h(\\phi)$ makes a correct predicition (same sign as the target), no update is made, whereas making an incorrect prediction causes $\\theta$ to be moved in the right direction to push the margin $y^{(i)}\\theta^T x^{(i)}$ towards a positive number.\n",
    "\n",
    "Now, if we initialize $\\theta = 0$, we will always have $\\theta = \\sum_{i=0}^n \\beta_i \\phi\\left(x^{(i)}\\right)$ after the first $n$ coefficients, so that predictions can be made just using the kernel: $h\\left(\\theta^T \\phi(x)\\right) = h\\left(\\sum_{i=0}^n \\beta_i K\\left(x^{(i)},x\\right)\\right)$.\n",
    "\n",
    "To train the perceptron, we initialize the coefficients ${\\beta_0,...\\beta_m} = 0$. Then, the $(n+1)$-th term is computed using the update rule and the kernel as $\\beta_{n+1} = y^{(n+1)}-h\\left(\\sum_{i=0}^{n} \\beta_i K\\left(x^{(i)},x\\right)\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spam classification**\n",
    "\n",
    "We'll tackle the problem of spam classification, first using Naive Bayes. First, let's make a function to read in the training and test data in the form of a document-word matrix, where the $(i,j)$-th element of the matrix indicates how many instances of word $j$ from our list of token occurs within document $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readDocWordMatrix(fileName):\n",
    "    file = open(fileName,'r')\n",
    "    file.readline() # meaningless header\n",
    "    [nDocs,nToks] = [int(s) for s in file.readline().split()]\n",
    "    docWordMat = np.zeros((nDocs,nToks))\n",
    "    tokens = file.readline().split()\n",
    "    categories = np.zeros(nDocs)\n",
    "    for idx,line in enumerate(file):\n",
    "        nums = [int(s) for s in line.split()]\n",
    "        categories[idx] = nums[0]\n",
    "        lastIdx = len(nums)-1\n",
    "        docWordMat[idx,np.cumsum(nums[1:lastIdx:2])] = nums[2:lastIdx:2]\n",
    "    file.close()\n",
    "    return docWordMat,tokens,categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compute the Naive Bayes parameters from the traiing set, using Laplace smoothing. Using the multinomial event model, the relevant parameters are then given by \n",
    "\n",
    "$\\phi_{k|y=1} = p(x_j=k|y=1) = \\frac{1+\\sum 1\\left\\{x_j^{(i)}=k \\wedge y^{(i)}=1\\right\\}}{|V|+\\sum 1\\left\\{y^{(i)}=1\\right\\}n_i}$,\n",
    "\n",
    "$\\phi_{k|y=0} = p(x_j=k|y=0) = \\frac{1+\\sum 1\\left\\{x_j^{(i)}=k \\wedge y^{(i)}=0\\right\\}}{|V|+\\sum 1\\left\\{y^{(i)}=0\\right\\}n_i}$,\n",
    "\n",
    "$\\phi_{y=1} = p(y=1) = \\frac{\\sum 1\\left\\{y^{(i)}=1\\right\\}}{m}$,\n",
    "\n",
    "where $|V|$ is the size of the vocabulary, $n_i$ is the total number of words in each training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNaiveBayes(file):\n",
    "    docWordMat,tokens,categories = readDocWordMatrix(file)\n",
    "    numWords = np.sum(docWordMat,axis=1)\n",
    "    numTokens = len(tokens)\n",
    "    likelihoodSpam = np.zeros(numTokens)\n",
    "    likelihoodNoSpam = np.zeros(numTokens)\n",
    "    probSpam= np.sum(categories)/len(categories)\n",
    "    for idx in range(numTokens):\n",
    "        likelihoodSpam[idx] = (np.sum(docWordMat[categories==1,idx])+1)/(np.sum(numWords[categories==1])+numTokens)\n",
    "        likelihoodNoSpam[idx] = (np.sum(docWordMat[categories==0,idx])+1)/(np.sum(numWords[categories==0])+numTokens)\n",
    "    return likelihoodSpam,likelihoodNoSpam,probSpam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "likelihoodSpam,likelihoodNoSpam,probSpam = trainNaiveBayes('MATRIX.TRAIN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we evaluate on the test set. For each document (row in the document-word matrix), we'll compute the conditional probability of seeing that word vector $x$ if the document is spam ($y = 1$) using the following formula:\n",
    "\n",
    "$p(x|y=1) = \\prod_{x_i} \\left(\\phi_{i|y=1}\\right)^{x_i}$\n",
    "\n",
    "and similarly for $y=0$. Then the probability of the document being spam is simply computed as \n",
    "\n",
    "$p(y=1|x) = \\frac{p(x|y=1)\\phi_{y=1}}{p(x|y=0)(1-\\phi_{y=1})+p(x|y=1)\\phi_{y=1}}$,\n",
    "\n",
    "With the e-mail being classified as spam if $p(y=1|x) \\geq 0.5$. \n",
    "\n",
    "The issue here is that most of the $\\phi_{i|y=1}$ and $\\phi_{i|y=0}$ are quite small, so we worry about underflow due to the product of so many small numbers. Thus, we look at the logarithm of the probability, converting the products to sums, and ignore the denominator (since it is only there for normalization). Thus, we compute\n",
    "\n",
    "$l_1 = ln\\left(p(x|y=1)\\phi_{y=1}\\right) = ln\\left(\\phi_{y=1}\\right) + \\sum_{x_i} x_i ln\\left(\\phi_{i|y=1}\\right)$,\n",
    "\n",
    "$l_0 = ln\\left(p(x|y=0)\\phi_{y=0}\\right) = ln\\left(1-\\phi_{y=1}\\right) + \\sum_{x_i} x_i ln\\left(\\phi_{i|y=0}\\right)$,\n",
    "\n",
    "and classify as spam if $e^{l_1} > e^{l_0}$, or equivalently $l_1 > l_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testNaiveBayes(likelihoodSpam,likelihoodNoSpam,probSpam):\n",
    "    docWordMatTest,tokensTest,categoriesTest = readDocWordMatrix('MATRIX.TEST')\n",
    "    numCorrect = 0\n",
    "    for testIdx,testLabel in enumerate(categoriesTest):\n",
    "        testVec = docWordMatTest[testIdx,:]\n",
    "        l1 = np.log(probSpam)+np.dot(testVec,np.log(likelihoodSpam))\n",
    "        l0 = np.log(1-probSpam)+np.dot(testVec,np.log(likelihoodNoSpam))\n",
    "        predictLabel = int(l1 >= l0)\n",
    "        numCorrect += int(predictLabel==int(testLabel))\n",
    "    print(\"Test error is \"+str(100*(1-numCorrect/len(categoriesTest)))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error is 1.6249999999999987%\n"
     ]
    }
   ],
   "source": [
    "testNaiveBayes(likelihoodSpam,likelihoodNoSpam,probSpam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also estimate the most likely spam words by looking at which words maximize the quantity $ln\\left(\\frac{\\phi_{k|y=1}}{\\phi_{k|y=0}}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['httpaddr', 'spam', 'unsubscrib', 'ebai', 'valet']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docWordMat,tokens,categories = readDocWordMatrix('MATRIX.TRAIN')\n",
    "wordLikelihood = np.log(likelihoodSpam/likelihoodNoSpam)\n",
    "np.argmax(wordLikelihood)\n",
    "sortedInds = np.argsort(wordLikelihood)[::-1]\n",
    "spamTokens = [tokens[s] for s in sortedInds[0:5]]\n",
    "spamTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see how the accuracy of the classifier converges with the amount of training data. In this case, we can see that even 50-100 documents is still quite good (~2x error compared to the full training set of over 2000 doucments), and the error has converged to that of a full training set already by ~1400 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size = 50 documents\n",
      "Test error is 3.874999999999995%\n",
      "Training size = 100 documents\n",
      "Test error is 2.6249999999999996%\n",
      "Training size = 200 documents\n",
      "Test error is 2.6249999999999996%\n",
      "Training size = 400 documents\n",
      "Test error is 1.8750000000000044%\n",
      "Training size = 800 documents\n",
      "Test error is 1.749999999999996%\n",
      "Training size = 1400 documents\n",
      "Test error is 1.6249999999999987%\n"
     ]
    }
   ],
   "source": [
    "trainingSize = ['50','100','200','400','800','1400']\n",
    "for size in trainingSize:\n",
    "    likelihoodSpam,likelihoodNoSpam,probSpam = trainNaiveBayes('MATRIX.TRAIN.'+size)\n",
    "    print('Training size = '+size+' documents')\n",
    "    testNaiveBayes(likelihoodSpam,likelihoodNoSpam,probSpam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spam classification 2 - SVM Boogaloo**\n",
    "\n",
    "Now we'll train a support vector machine spam classifier. Rather than using coordinate descent using sequential minimal optimization, we'll use stochastic gradient descent on the regularized empirical loss function\n",
    "\n",
    "$J(\\alpha) = \\frac{1}{m}\\sum\\left[1-y^{(i)}K^{(i),T}\\alpha\\right]_+ +\\frac{\\lambda}{2}\\alpha^T K \\alpha$,\n",
    "\n",
    "where $K^{i}$ is the $i$-th column of the kernel matrix, $[x]_+ = max\\{0,x\\}$, and $lambda$ is the regularization constant. Technically, since the loss function is not differentiable at 0 the gradient descent is sub-gradient descent, but whatevs. It's not mentioned why we're doing things this way vs coordinate descent using sequential minimal optimization. Wikipedia mentions that SGD can be more advantageous compared to SMO if the number of training examples is quite large (since we can change all parameters of $\\alpha$ at once each iteration) and vice versa if the space is very high dimensional (again, since we change *all* parameters of $\\alpha$ at once in SGD). This makes sense, though here I doubt it makes a huge difference. Whatever.\n",
    "\n",
    "For the kernel we're going to use the radial basis function\n",
    "\n",
    "$K(x,z) = exp\\left(\\frac{1}{2\\tau^2}|x-z|^2\\right)$,\n",
    "\n",
    "because Gaussians are easy and magic, and we're given values of all the constant parameters to use because there's no time for shenanigans. We'll also use a step size of $\\frac{1}{\\sqrt{t+1}}$, where $t$ is the iteration number. No reason has been given from this (aside from some comments that it is a typical choice), but some sort of learning rate schedule is typical to give a trade-off between constant large changes to $\\alpha$ that cause instability and super slow convergence. I guess this is just a very simple choice which is good enough. Still not sure why $\\sqrt{t}$ and not $t$, though...\n",
    "\n",
    "Some other quirks specified without explanation: We'll adjust the document-word matrix so that columns are only 0 or 1 (i.e., we only care which words appear, not how often within a document). Not sure why, aside from reasons of feature scaling for stability reasons. We're also going to do *averaged* SGD, where we average all the $\\alpha$ from each step together at the end and report that average value as the result, rather than the final value of $\\alpha$.\n",
    "\n",
    "Okay, enough talk. Now is the time for action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainAndTestSVM(file,tau=8,iterMult=40,regParam=1/64):\n",
    "    # Import document-word matrix, convert training labels to {1,-1} and convert feature\n",
    "    # vectors to {0,1} vectors\n",
    "    docWordMat,tokens,yTrain = readDocWordMatrix(file)\n",
    "    numTrain = len(yTrain)\n",
    "    yTrain[yTrain<1] = -1\n",
    "    xTrain = (docWordMat>0).astype(int)\n",
    "    \n",
    "    # Compute the kernel for the training data using the radial basis function\n",
    "    xTrainSq = np.tile(np.sum(xTrain**2,axis=1)[:,None],numTrain)\n",
    "    kTrain = np.exp(-(xTrainSq+xTrainSq.T-2*np.dot(xTrain,xTrain.T))/(2*tau**2))\n",
    "\n",
    "    # Stochastic gradient descent for fixed number of iterations\n",
    "    numIter = iterMult*numTrain\n",
    "    alpha = np.zeros(numTrain)\n",
    "    avgAlpha = np.zeros(numTrain)\n",
    "    for idx in range(numIter):\n",
    "        randIdx = randint(0,high=numTrain)\n",
    "        margin = yTrain[randIdx]*np.dot(kTrain[:,randIdx],alpha)\n",
    "        grad = regParam*alpha[randIdx]*kTrain[:,randIdx]-int(margin<1)*yTrain[randIdx]*kTrain[:,randIdx]\n",
    "        alpha -= grad/np.sqrt(idx+1)\n",
    "        avgAlpha += alpha\n",
    "    avgAlpha /= numTrain\n",
    "    \n",
    "    # Import document-word matrix, convert training labels to {1,-1} and convert feature\n",
    "    # vectors to {0,1} vectors\n",
    "    docWordMat,tokens,yTest = readDocWordMatrix('MATRIX.TEST')\n",
    "    numTest = len(yTest)\n",
    "    yTest[yTest<1] = -1\n",
    "    xTest = (docWordMat>0).astype(int)\n",
    "    \n",
    "    # Compute the kernel for the test data using the radial basis function\n",
    "    xTestSq = np.tile(np.sum(xTest**2,axis=1)[:,None],numTrain)\n",
    "    xTrainSq2 = np.tile(np.sum(xTrain**2,axis=1)[:,None],numTest)\n",
    "    kTest = np.exp(-(xTestSq+xTrainSq2.T-2*np.dot(xTest,xTrain.T))/(2*tau**2))\n",
    "    \n",
    "    predictions = np.dot(kTest,alpha)\n",
    "    numErrors = np.sum(predictions*yTest<=0)\n",
    "    return numErrors/numTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error is 3.375%\n"
     ]
    }
   ],
   "source": [
    "print('Training error is '+str(100*trainAndTestSVM('MATRIX.TRAIN.50'))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size = 50 documents\n",
      "Avg. training error is 8.4625%\n",
      "Training size = 100 documents\n",
      "Avg. training error is 4.925%\n",
      "Training size = 200 documents\n",
      "Avg. training error is 2.2375%\n",
      "Training size = 400 documents\n",
      "Avg. training error is 1.7625%\n",
      "Training size = 800 documents\n",
      "Avg. training error is 0.3875%\n",
      "Training size = 1400 documents\n",
      "Avg. training error is 0.0375%\n"
     ]
    }
   ],
   "source": [
    "numAvgErrors = 10\n",
    "for size in trainingSize:\n",
    "    print('Training size = '+size+' documents')\n",
    "    avgError = 0\n",
    "    for idx in range(numAvgErrors):\n",
    "        avgError += trainAndTestSVM('MATRIX.TRAIN.'+size)\n",
    "    print('Avg. training error is '+str(100*avgError/numAvgErrors)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That took *much* longer to train than Naive Bayes, but is clearly superior. Notice that while we actually have worse average performance for a small training set, by the time the training set is only 200 documents we have converged to a performance better than Naive Bayes on the full 2000 document training set, and our best error is almost an order of magnitude smaller than the best Naive Bayes error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and testing on different distributions**\n",
    "\n",
    "Consider binary classificaion with two distributions: $D$, the \"clean\" distribution and $D_\\tau$, a distribution identical to $D$ except with a probability $0 \\leq \\tau \\lt 0.5$ of the label being flipped (e.g., whoever labeled the data screws up from time to time).\n",
    "\n",
    "For a given hypothesis $h$, we now have two error quantities: $\\epsilon_0$, the \"true\" generalization error on $D$, and $\\epsilon_\\tau$, the generalization error when drawing samples from $D_\\tau$\n",
    "\n",
    "(a) How are these two related? Note that for the hypothesis to make an error when drawing from $D_\\tau$, one of two things must happen. Either the hypothesis would have made the correct prediction when drawing from $D$ (probability $1-\\epsilon_0$ but the label is flipped (probability $\\tau$), or we would have gotten the wrong answer if drawing from $D$ and the label was not flipped when drawing from $D_\\tau$. Thus\n",
    "\n",
    "$\\epsilon_\\tau = \\epsilon_0 (1-\\tau)+(1-\\epsilon_0)\\tau \\rightarrow \\epsilon_0 = \\left(\\epsilon_\\tau-\\tau\\right)/(1-2\\tau)$.\n",
    "\n",
    "(b) Now, consider a set of hypotheses of size $|H|$, and let's assume our training is done via empirical risk minimization on the corrupted set $S$ drawn from $D_\\tau$, but we test against $D$. Thus, we have the hypothesis $\\hat{h}$, which minimizes the empirical risk in training $\\hat{\\epsilon}_S(h)$ and the true best hypothesis $h^*$ which minimizes the generalization error $\\epsilon_0(h)$.\n",
    "\n",
    "Now, if the uniform convergence result relating empirical and generalization error holds, that is \n",
    "\n",
    "$|\\hat{\\epsilon}_\\tau(h)-\\epsilon_\\tau(h)| \\leq \\tilde{\\gamma}$,\n",
    "\n",
    "we know that\n",
    "\n",
    "$\\epsilon_\\tau(\\hat{h}) \\leq \\epsilon_\\tau(h^*) + 2\\tilde{\\gamma}$. This will be true with probability $1-\\delta$, where $\\delta = 2|H|exp(-2\\tilde{\\gamma}^2 m)$.\n",
    "\n",
    "Using our earlier relation between $\\epsilon_0(h)$ and $\\epsilon_\\tau(h)$, we have\n",
    "\n",
    "$\\epsilon_0(\\hat{h}) = \\left(\\epsilon_\\tau(\\hat{h})-\\tau\\right)/(1-2\\tau)$.\n",
    "\n",
    "Then, with probability $1-\\delta$ uniform convergence holds and\n",
    "\n",
    "$\\epsilon_0(\\hat{h}) \\leq \\left(\\epsilon_\\tau(h^*)+2\\tilde{\\gamma}-\\tau\\right)/(1-2\\tau)$.\n",
    "\n",
    "Applying the relation between $\\epsilon$s once again, we get\n",
    "\n",
    "$\\epsilon_0(\\hat{h}) \\leq \\epsilon_0(h^*)+2\\tilde{\\gamma}/(1-2\\tau)$.\n",
    "\n",
    "So, for fixed $\\delta$,$\\gamma$, a uniform convergence result \n",
    "\n",
    "$\\epsilon_0(\\hat{h}) \\leq \\epsilon_0(h^*)+2\\gamma$\n",
    "\n",
    "holds with probability $1-\\delta$ holds even when training on the corrupted distribution provided\n",
    "\n",
    "$\\delta = 2|H|exp(-2\\tilde{\\gamma}^2 m)$, $\\tilde{\\gamma} = \\gamma(1-2\\tau)$,\n",
    "\n",
    "or\n",
    "\n",
    "$m \\geq \\frac{1}{2\\gamma^2 (1-2\\tau)^2} log(2|H|/\\delta)$.\n",
    "\n",
    "In other words, uniform convergence looks pretty much the same, but to get the same generalization error bounds with the same probability we need to use a lot more training data to counteract the probability of getting bad samples. In particular, every $m$ training samples from the corrupted distribution is like having $(1-2\\tau)^2 m$ samples from the clean distribution.\n",
    "\n",
    "(c) Note that this makes it clear why we are requiring $\\tau \\neq 0.5$. As $\\tau \\rightarrow 0.5$, the number of training samples required to get a bound on the generalization error diverges. This makes sense looking at the relation in (a): when $\\tau = 0.5$, the generalization error $\\epsilon_\\tau = 0.5$. In other words, there is no relation between the generalization error on the corrupted distribution and that on the clean distribution. So, we can't make any guarantees on a bound for the generalization error when testing on the clean distribution, no matter how many samples we draw from the corrupted distribution during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Boosting+Fizzix = â˜º**\n",
    "\n",
    "Consider decision stumps as an example of weak learners. For a feature $x$, a decision stump is a hypothesis $\\phi_{s,\\pm}$ defined to be\n",
    "\n",
    "$\\phi_{s,\\pm} = \\pm \\text{sign}(x-s)$.\n",
    "\n",
    "Obviously, in a real problem we'll have multiple features $x_j$, which may not even be real valued, so we'll have an additional degree of freedom in picking a decision stump, but for now we'll consider a single feature $x \\in \\mathbb{R}$.\n",
    "\n",
    "Now, we consider a training set $\\left\\{y^{(i)},x^{(i)}\\right\\}$ with labels $y^{(i)} \\in \\left\\{-1,1\\right\\}$, and a distribution $p_i$ on the training set with $\\sum p_i = 1$, $p_i \\geq 0$. For $\\phi_{s,\\pm}$ to be a weak learner on this training set/distribution we just need\n",
    "\n",
    "$\\sum p_i 1\\left\\{y^{(i)}\\phi_{s,\\pm}<0\\right\\} \\leq \\frac{1}{2}-\\gamma$,\n",
    "\n",
    "for $\\gamma \\gt 0$. For simplicity we'll assume all $x^{(i)}$ are distinct and ordered such that $x^{(i)} > x^{(i+1)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Suppose we have such a decision stump. Show that for $\\forall s, \\exists m_0(s) \\in \\left\\{1,2,...,m\\right\\}$ such that\n",
    "\n",
    "$\\sum p_i 1\\left\\{y^{(i)}\\phi_{s,\\pm}<0\\right\\} = \\frac{1}{2} \\mp \\frac{1}{2}\\left(\\sum_{i=1}^{m_0}y^{(i)}p_i - \\sum_{i=m_0+1}^{m} y^{(i)} p_i\\right)$\n",
    "\n",
    "Note that, given that the training features are ordered in descending order, for any threshold $s$ we can choose an index $m_0$ such that $x^{(i)} \\geq s$, $i \\leq m_0$, and $x^{(i)} \\lt s$, $i \\gt m_0$.\n",
    "\n",
    "Using this choice for $m_0$, we can see that\n",
    "\n",
    "$\\sum_{i=1}^{m_0} p_i 1\\left\\{y^{(i)}\\phi_{s,\\pm}<0\\right\\} = \\frac{1}{2}\\sum_{i=1}^{m_0} p_i\\left(1\\mp y^{(i)}\\right)$,\n",
    "\n",
    "since the examples are only mislabeled when $y^{(i)} = \\mp1$, and these are the only examples which contribute a non-zero amount to the right hand sum. By similar logic, we see\n",
    "\n",
    "$\\sum_{i=m_0+1}^{m} p_i 1\\left\\{y^{(i)}\\phi_{s,\\pm}<0\\right\\} = \\frac{1}{2}\\sum_{i=m_0+1}^{m} p_i\\left(1\\pm y^{(i)}\\right)$.\n",
    "\n",
    "Summing these two expressions and using the fact that $\\sum p_i = 1$, we recover the desired expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Defining the function\n",
    "\n",
    "$f(m_0) = \\sum_{i=1}^{m_0}y^{(i)}p_i - \\sum_{i=m_0+1}^{m} y^{(i)} p_i$,\n",
    "\n",
    "show that $\\exists \\gamma \\gt 0$ such that for any distribution $p$ on the training set we can define $m_0$ with\n",
    "\n",
    "$\\left|f(m_0)\\right| \\geq 2\\gamma$.\n",
    "\n",
    "\n",
    "Note that $f(m_0)-f(m_0-1) = 2y^{(m_0-1)}p_{m_0-1}$, so that $\\left|f(m_0)-f(m_0-1)\\right| = 2p_{m_0-1}$. Since $\\sum p_i = 1$, $\\forall p$, $\\exists p_i \\geq 1/m$. Thus, we can always choose $m_0$ such that $\\left|f(m_0)-f(m_0-1)\\right| = 2/m$. In this case, it must be true that either $\\left|f(m_0)\\right|$ or $\\left|f(m_0-1)\\right|$ is greater than or equal to $1/m$, so $\\gamma = 1/2m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Based on the answers above, what *edge*, that is, what $\\gamma \\in [0,1/2]$ such that\n",
    "\n",
    "$\\sum p_i 1\\left\\{y^{(i)}\\phi<0\\right\\} \\leq \\frac{1}{2}-\\gamma$,\n",
    "\n",
    "can decision stumps of this form guarantee on a training set where the features are all distinct?\n",
    "\n",
    "Recall that in part (a) we showed that given a threshold $s$ we can choose an index $m_0$ such that\n",
    "\n",
    "$\\sum p_i 1\\left\\{y^{(i)}\\phi_{s,\\pm}<0\\right\\} = \\frac{1}{2} \\mp \\frac{1}{2}\\left|f(m_0)\\right|$.\n",
    "\n",
    "In part (b), we showed that for any distribution $p$ we can find an index $m_0$ such that $|f(m_0)| \\geq 1/m$. Thus, for any training set and distribution we can always pick such an $m_0$, and choose a corresponding threshold $s$ to satisfy the conditions in part (a). In that case, we choose either $\\phi_{s,+}$ or $\\phi_{s,-}$ (depending on the sign of $f(m_0)$) and are guaranteed that \n",
    "\n",
    "$\\sum p_i 1\\left\\{y^{(i)}\\phi<0\\right\\} \\leq \\frac{1}{2}-\\frac{1}{2m}$,\n",
    "\n",
    "so that we can always guarantee an edge of $\\gamma \\geq \\frac{1}{2m}$.\n",
    "\n",
    "Recall that boosting (using *any* weak learner) converges to zero training error after a number of iterations $t = \\frac{\\text{log}(m)}{2\\gamma^2}$, so this bound on $\\gamma$ guarantees the number of iterations satsfies\n",
    "\n",
    "$t \\leq 2 m^2\\text{log}(m)$,\n",
    "\n",
    "and consequently we'll need at most $2 m^2\\text{log}(m)$ decision stumps to make a classifier with zero training error$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Now, let's actually implement boosting using decision stumps! First, we obviously need the function which implements a simple decision stump, as well as the function which runs the actual classfier: evaluating $\\sum \\theta_\\tau \\phi_\\tau$ for all stumps $\\phi_\\tau$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stump(x,featureInds,thresholds,signs):\n",
    "    return np.sign(x[:,featureInds]-thresholds)*signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalStumps(x,thetas,featureInds,thresholds,signs):\n",
    "    if thetas.size==0:\n",
    "        return np.zeros(x.shape[0])\n",
    "    return np.dot(stump(x,featureInds,thresholds,signs),thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's make a function to determine the optimal threshold and feature index for a given set of training data and a given distribution on that training data. How to do this most efficiently? Well, let's consider the problem of finding the best threshold when the index $j$ is fixed. First, let's sort all features $x^{(i)}_j$ across all training examples such that\n",
    "\n",
    "$x^{(1)}_j \\leq x^{(2)}_j \\leq ... \\leq x^{(m)}_j$.\n",
    "\n",
    "Now, set aside the issue of the exact threshold value for the moment and consider that we have $m+1$ possible choices of threshold $s_k$, $k \\in \\{0,1,...,m\\}$, where $x^{(k)}_j \\leq s_k \\leq x^{(k+1)}$. Note that in this case we can express the error for threshold $s_k$ as\n",
    "\n",
    "$\\epsilon_k = \\sum_{i=1}^{k} p_i 1\\left\\{y^{(i)}=1\\right\\} + \\sum_{i=k+1}^{m} p_i 1\\left\\{y^{(i)}=-1\\right\\}$.\n",
    "\n",
    "The difference between the error for two consecutive thresholds is thus\n",
    "\n",
    "$\\epsilon_{k+1}-\\epsilon_k = p_{k+1} 1\\left\\{y^{(k+1)}=1\\right\\} - p_{k+1} 1\\left\\{y^{(k+1)}=-1\\right\\}$,\n",
    "\n",
    "giving us the recurrence relation\n",
    "\n",
    "$\\epsilon_{k+1} = \\epsilon_k + p_{k+1}y^{(k+1)}$, $\\epsilon_0 = \\sum p_i 1\\left\\{y^{(i)}=-1\\right\\}$.\n",
    "\n",
    "So, after sorting the training examples for a given feature $x_j$, we can merely iterate over all examples and compute the sequence $\\epsilon_k$, then find the minimum value, all in $O(m)$ time. The sort takes $O(m\\text{log}(m))$ time, and we'll need to iterate over all $n$ features, so the overall run time will be $O(nm\\text{log}(m))$. We'll also keep track of the minimum of $1-\\epsilon_k$. If this is smallest we'll use the opposite sign decision stump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBestThreshold(x,y,p):\n",
    "    bestErr,bestJ,bestS,bestSign = 1,0,0,1\n",
    "    for jdx,feature in enumerate(x.T):\n",
    "        # sort labels and distribution by feature value\n",
    "        sortedIndices = np.argsort(feature)\n",
    "        fSort = feature[sortedIndices]\n",
    "        ySort = y[sortedIndices]\n",
    "        pSort = p[sortedIndices]\n",
    "        \n",
    "        # iteratively compute the total error for each possible threshold\n",
    "        err = [np.sum(pSort[ySort<0])]\n",
    "        for yK,pK in zip(ySort,pSort):\n",
    "            err.append(err[-1]+pK*yK)\n",
    "        minusErr = [1-e for e in err]\n",
    "        \n",
    "        # pick the best of the error or 1-error\n",
    "        currIdx = np.argmin(err)\n",
    "        currErr = err[currIdx]\n",
    "        currSign = 1\n",
    "        currIdx2 = np.argmin(minusErr)\n",
    "        if minusErr[currIdx2]<currErr:\n",
    "            currIdx = currIdx2\n",
    "            currErr = minusErr[currIdx]\n",
    "            currSign = -1\n",
    "        \n",
    "        # choose the threshold \n",
    "        if currIdx==0:\n",
    "            currS = fSort[0]-1\n",
    "        elif currIdx==fSort.size:\n",
    "            currS = fSort[-1]+1\n",
    "        else:\n",
    "            currS = 0.5*(fSort[currIdx]+fSort[currIdx+1])\n",
    "            \n",
    "        if currErr<bestErr:\n",
    "            bestErr = currErr\n",
    "            bestJ = jdx\n",
    "            bestS = currS\n",
    "            bestSign = currSign\n",
    "    \n",
    "    return bestJ,bestS,bestSign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement the function which will run the actual boosting algorithm: finding the best decision stump for the current distribution, and selecting its weight in the sum of weak learners, then updating the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stumpBooster(x,y,t):\n",
    "    # arrays containing the classfier data\n",
    "    thetas,featureInds,thresholds,signs = np.zeros(t),[0]*t,np.zeros(t),np.zeros(t)\n",
    "    for idx in range(t):\n",
    "        # compute weights and distribution\n",
    "        weights = np.exp(-y*evalStumps(x,thetas,featureInds,thresholds,signs))\n",
    "        p = weights/np.sum(weights)\n",
    "\n",
    "        j,s,sign = findBestThreshold(x,y,p)\n",
    "        featureInds[idx] = j\n",
    "        thresholds[idx] = s\n",
    "        signs[idx] = sign\n",
    "        \n",
    "        margins = y*stump(x,j,s,sign)\n",
    "        Wplus = np.sum(weights[margins>0])\n",
    "        Wminus = np.sum(weights[margins<0])\n",
    "        thetas[idx] = 0.5*np.log(Wplus/Wminus)\n",
    "        \n",
    "    return thetas,featureInds,thresholds,signs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll implement *random* boosting. For this, we'll iterate $t$ times, but at each iteration we simply choose a random feature index and corresponding random threshold (and we'll just stick with positive decision stumps $\\phi_{s.+}$), and choose it's coefficient $\\theta_t$ optimally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomBooster(x,y,t):\n",
    "    nSamples,nFeatures = x.shape\n",
    "    # arrays containing the classfier data\n",
    "    thetas,featureInds,thresholds,signs = np.zeros(t),[0]*t,np.zeros(t),np.ones(t)\n",
    "    for idx in range(t):\n",
    "        # compute weights and distribution\n",
    "        weights = np.exp(-y*evalStumps(x,thetas,featureInds,thresholds,signs))\n",
    "        p = weights/np.sum(weights)\n",
    "\n",
    "        j = randint(nFeatures)\n",
    "        fSort = np.sort(x[:,j])\n",
    "        sIdx = randint(nSamples+1)\n",
    "        if sIdx==0:\n",
    "            s = fSort[0]-1\n",
    "        elif sIdx==fSort.size:\n",
    "            s = fSort[-1]+1\n",
    "        else:\n",
    "            s = 0.5*(fSort[sIdx]+fSort[sIdx+1])\n",
    "        featureInds[idx] = j\n",
    "        thresholds[idx] = s\n",
    "        \n",
    "        margins = y*stump(x,j,s,1)\n",
    "        Wplus = np.sum(weights[margins>0])\n",
    "        Wminus = np.sum(weights[margins<0])\n",
    "        thetas[idx] = 0.5*np.log(Wplus/Wminus)\n",
    "        \n",
    "    return thetas,featureInds,thresholds,signs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make a function to plot error over time. That is, given a boosting algorithm that ran for $t$ iterations, we'll compute the success rate on the full training and testing sets including the first $n$ decision stumps ($1 \\lt n \\leq t$) and plot the results as a function of $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotErrorOverTime(xTrain,yTrain,xTest,yTest,thetas,featureInds,thresholds,signs):\n",
    "    trainIter = thetas.size\n",
    "    trainErrors,testErrors = [],[]\n",
    "    trainPredictions,testPredictions = np.zeros(yTrain.size),np.zeros(yTest.size)\n",
    "    \n",
    "    for theta,j,s,sign in zip(thetas,featureInds,thresholds,signs):\n",
    "        trainPredictions += evalStumps(xTrain,theta,j,s,sign)\n",
    "        testPredictions += evalStumps(xTest,theta,j,s,sign)\n",
    "        trainErrors.append(np.sum(yTrain*trainPredictions<=0)/yTrain.size)\n",
    "        testErrors.append(np.sum(yTest*testPredictions<=0)/yTest.size)\n",
    "    \n",
    "    plt.plot(trainErrors,c='b',label='Train errors')\n",
    "    plt.plot(testErrors,c='k',label='Test errors')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll read in the full testing and training sets and plot the errors over time for using optimal decision stump boosting and random decision stump boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "boostingTrain = np.array(list(csv.reader(open('boosting-train.csv','r'),delimiter=',',quoting=csv.QUOTE_NONNUMERIC)))\n",
    "labelsTrain = boostingTrain[:,0]\n",
    "featuresTrain = boostingTrain[:,1:]\n",
    "\n",
    "boostingTest = np.array(list(csv.reader(open('boosting-test.csv','r'),delimiter=',',quoting=csv.QUOTE_NONNUMERIC)))\n",
    "labelsTest = boostingTest[:,0]\n",
    "featuresTest = boostingTest[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEATBgRDRFwRBcCD+H9YDAIUU7aKrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f608c04d550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "thet,featInd,threshs,sgns = stumpBooster(featuresTrain,labelsTrain,200)\n",
    "plotErrorOverTime(featuresTrain,labelsTrain,featuresTest,labelsTest,thet,featInd,threshs,sgns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl4FFXa9/HvydrZIOwgWzBhUBYN\nIaCAMigqiAqKijiisjgIDiqivsLzqCCOM6KPiogOMmyjoOiIKCAKIiKi7JKwigQUCGsSCFnIQqfv\n94/qxE5ISCeE7pi+P9eVi+7qU9V3V8KvT5+qOm1EBKWUUr7Bz9sFKKWU8hwNfaWU8iEa+kop5UM0\n9JVSyodo6CullA/R0FdKKR+ioa+UUj5EQ18ppXyIhr5SSvmQAG8XUFL9+vUlKirK22UopdQfypYt\nW1JFpEF57apd6EdFRbF582Zvl6GUUn8oxpgD7rTT4R2llPIhGvpKKeVDNPSVUsqHVLsxfaXUH8fZ\ns2dJTk4mNzfX26X4DJvNRrNmzQgMDKzU+hr6SqlKS05OJiIigqioKIwx3i6nxhMR0tLSSE5OplWr\nVpXahg7vKKUqLTc3l3r16mnge4gxhnr16l3QJysNfaXUBdHA96wL3d81JvQzMzOZMGECGzdu9HYp\nSilVbdWY0M/Ly2PSpEls2LDB26UopTwkLS2N2NhYYmNjady4MU2bNi26n5+f79Y2hg4dyp49ey5y\npdVHjTmQa7PZACv8lVK+oV69eiQkJAAwceJEwsPDeeqpp4q1ERFEBD+/0vu4c+bMqfK67HY7AQEB\nZd4vS3m1VgW3tmyM6WOM2WOMSTLGjCvl8SHGmBRjTILz5yGXxx40xux1/jxYlcW7Cg4OBtBTx5RS\nJCUl0b59e0aOHElcXBxHjx5lxIgRxMfH065dOyZNmlTU9pprriEhIQG73U5kZCTjxo3jyiuvpGvX\nrpw4ceKcbWdlZTFkyBC6dOlCx44dWbJkCQAzZ85k0KBB3Hrrrdx8882sXLmSG264gUGDBtGxY0cA\nXnnlFdq3b0/79u156623Sq310KFD3H///XTo0IH27dszderUKt035b71GGP8gbeBG4FkYJMxZrGI\n7CrR9CMRGV1i3brABCAeEGCLc91TVVK9i4CAAPz8/DT0lfKSMWPA2emuMrGxMGVK5dbdtWsXc+bM\nYfr06QC8/PLL1K1bF7vdznXXXcddd91F27Zti61z+vRp/vznP/Pyyy8zduxYZs+ezbhxxfu5kyZN\nok+fPsydO5dTp05x1VVXceONNwKwbt06EhISqFOnDitXrmT9+vXs2rWLFi1asHHjRubPn8/GjRsp\nKCigS5cu/PnPfyY0NLRYrRs2bCA1NZXt27cDkJ6eXrkdUAZ3evpdgCQR2S8i+cACoL+b2+8NfC0i\nJ51B/zXQp3Klnp8xBpvNpsM7SikAoqOj6dy5c9H9Dz/8kLi4OOLi4ti9eze7dpXst0JISAg333wz\nAJ06deK33347p82KFSt46aWXiI2N5brrriM3N5eDBw8CcNNNN1GnTp2itl27dqVFixYAfP/999x5\n552EhoYSERHB7bffztq1a8+pNSYmhj179vD444+zfPlyateuXTU7xMmdMf2mwCGX+8nAVaW0u9MY\n0wP4BXhCRA6VsW7TStZaLpvNRk6O9vSV8obK9sgvlrCwsKLbe/fu5c0332Tjxo1ERkYyePDgUkcF\ngoKCim77+/tjt9vPaSMifPbZZ0RHRxdbvmbNmmLPWbIGEXGr1nr16rFt2za+/PJLpk6dysKFC5kx\nY8Z5XmnFuNPTL+2k0JLVLwGiROQKYCXwnwqsizFmhDFmszFmc0pKihslnev4cTh5MpiEBA19pVRx\nGRkZREREUKtWLY4ePcry5csrva3evXsXG2ffunWrW+v16NGDRYsWkZOTQ1ZWFp9//jnXXnvtOe1S\nUlIQEe6++25eeOEFfvrpp0rXWhp3evrJQHOX+82AI64NRCTN5e6/gcku6/Ysse7qkk8gIjOAGQDx\n8fFlvx2eh3Xyjk3H9JVS54iLi6Nt27a0b9+eSy+9lO7du1d6WxMmTGDMmDF06NABh8NBTEwMn3/+\nebnrdenShXvvvbdoGGfUqFF06NCBpKSkYu0OHTrE8OHDERGMMUyePLm0zVWaOd9HDgBjTADWkE0v\n4DCwCfiLiOx0adNERI46b98BPCMiVzsP5G4B4pxNfwI6icjJsp4vPj5eKvMlKnl5YLNZv9Tt2z+u\n8PpKqYrbvXs3l19+ubfL8Dml7XdjzBYRiS9v3XJ7+iJiN8aMBpYD/sBsEdlpjJkEbBaRxcBjxph+\ngB04CQxxrnvSGPMi1hsFwKTzBf6FsIbigsnL056+UkqVxa2Ls0RkGbCsxLLnXW6PB8aXse5sYPYF\n1OgWY8AYG/n5GvpKKVWWGjMNA4C/v42zZ/WUTaWUKksNDH3t6SulVFlqWOgHa+grpdR51KjQDwiw\nYbdr6CulVFlqVOgHBtooKNAxfaV8RVVMrQwwe/Zsjh07dhErrT5qzNTKAAEBwRQUaE9fKV/hztTK\n7pg9ezZxcXE0bty4UnVUdipld9tVpRoV+lZPX0NfKQX/+c9/ePvtt8nPz6dbt25MmzYNh8PB0KFD\nSUhIQEQYMWIEjRo1IiEhgXvuuYeQkBA2btxYbA6evXv3Mnr0aFJTUwkLC2PmzJn86U9/YvDgwTRq\n1IiffvqJzp07ExQUREpKCvv376dx48bMmDGDkSNH8tNPPxEYGMiUKVPo0aMHM2fOZOXKlWRlZZGX\nl8fcuXO55557yMrKwm63M2PGDLp163bR9kuNCv3gYBsOhw7vKOUNY8aMKep1V5XY2FimVGImtx07\ndrBo0SJ+/PFHAgICGDFiBAsWLCA6OvqcaYsjIyN56623mDZtGrGxsedsa8SIEcycOZPo6Gh++OEH\nRo8ezYoVKwDYt28f33zzDX5+fjz77LNs3bqVNWvWYLPZmDx5MkFBQWzfvp2dO3fSt29f9u7dCxSf\ngnny5MncdtttPPPMMxQUFJCTk3MBe6x8NSr0g4JsiOQWzVmhlPJNK1euZNOmTcTHW7MS5OTk0Lx5\nc3r37l00bXHfvn256aabzrud9PR01q9fz5133lm0zHXmzbvvvrvYt1z179+/6Fv81q5dy9NPPw1A\nu3btuOSSS4rm2XGdgrlz5848/PDD5Obmcvvtt3PllVdWwR4oW40K/cJvz8rPzy+6rZTyjMr0yC8W\nEWHYsGG8+OKL5zxWkWmLRYT69euX+QmmKqZSvv7661m9ejVffPEF9913H+PHj+e+++4rc90LVaPO\n3gkOtt5hdaZNpXzbDTfcwMcff0xqaipgneVz8ODBMqctjoiIIDMz85zt1KlThyZNmrBo0SIAHA4H\niYmJbtXQo0cP5s+fD1gTpB09epSYmJhz2h04cIDGjRszYsQIhgwZ4vZUzZVVo3r6+uXoSimADh06\nMGHCBG644QYcDgeBgYFMnz4df3//UqctHjp0KA899FCpB3IXLFjAqFGjmDhxIvn5+QwePNitIZhH\nH32Uhx9+mA4dOhAYGMh7771XbLuFvvnmG15//XUCAwMJDw9n3rx5VbcjSlHu1MqeVtmplQFuvnk2\nX301nAMHDhR9RZlS6uLRqZW940KmVq5RwzshIdY4vn5lolJKla6Ghb41vJOZqcM7SilVmhoV+qGh\nVuhnZGhPXylPqW5DxDXdhe7vGhb61vCOhr5SnmGz2UhLS9Pg9xARIS0treiklcqoUWfvhIVpT18p\nT2rWrBnJycmkpKR4uxSfYbPZaNasWaXXr1GhHx5uhX52to7pK+UJgYGBtGrVyttlqAqoUcM7haGf\nlaU9faWUKk0NC31rTF9DXymlSlejQr9WLR3eUUqp86lRoR8RYYX+mTPa01dKqdLUqNCvVcsa3snO\n1tBXSqnS1LDQt3r6Og2DUkqVrkaFfkREIGDIydExfaWUKo1boW+M6WOM2WOMSTLGjDtPu7uMMWKM\niXfejzLG5BhjEpw/06uq8NKEhhrApj19pZQqQ7kXZxlj/IG3gRuBZGCTMWaxiOwq0S4CeAzYUGIT\n+0Tk3C+evAisK5OD9UtUlFKqDO709LsASSKyX0TygQVA/1LavQi8Angtca3Qt+mXqCilVBncCf2m\nwCGX+8nOZUWMMR2B5iKytJT1WxljthpjvjPGXFvaExhjRhhjNhtjNl/IHB4BAWCFvvb0lVKqNO6E\nvillWdGUesYYP+AN4MlS2h0FWohIR2As8IExptY5GxOZISLxIhLfoEED9yovq1ijoa+UUmVxJ/ST\ngeYu95sBR1zuRwDtgdXGmN+Aq4HFxph4EckTkTQAEdkC7AP+VBWFl8XPL5j8fA19pZQqjTuhvwlo\nbYxpZYwJAgYBiwsfFJHTIlJfRKJEJApYD/QTkc3GmAbOA8EYYy4FWgP7q/xVuPD3t3H2rI7pK6VU\naco9e0dE7MaY0cBywB+YLSI7jTGTgM0isvg8q/cAJhlj7EABMFJETlZF4WWxQl97+kopVRq35tMX\nkWXAshLLni+jbU+X2wuBhRdQX4X5+wdz9uxpTz6lUkr9YdSoK3IBAgJs2O06vKOUUqWpcaEfGGij\noECHd5RSqjQa+kop5UNqYOgHU1CgwztKKVWaGvXF6ABBQTbs9lRuvfVWAGJiYnjjjTcwprRrzJRS\nyrfUuJ5+y5Y3ExQUy7Fjx/j5559588032bt3r7fLUkqpaqHGhX5MzM2EhW1k+vTNvPrqVwB88803\nXq5KKaWqhxoX+nXrwqlT0LkzDBgQjTEt+Oyzld4uSymlqoUaN6Y/bhx07w4OB+TnG+65pxerVn2G\n3V5AQIC/t8tTSimvqnE9/YgI6NsXbr0VBgyAe+/thd1+iilTErxdmlJKeV2NC/2S/vnPXgAsWbLK\ny5UopZT31fjQb9q0MRBBRsZRb5eilFJeV+NDH8DPz6Zz7CulFD4S+sZo6CulFPhI6Pv56Rz7SikF\nGvpKKeVTfCL0/f1t2O0a+koppaGvlFI+RENfKaV8iE+EfkCAfrGKUkqBhr5SSvkUnwj9wEAbDoeG\nvlJK+Uzoa09fKaV8KPRFNPSVUsonQj8oSENfKaXAzdA3xvQxxuwxxiQZY8adp91dxhgxxsS7LBvv\nXG+PMaZ3VRRdUcHBVuiLiDeeXimlqo1yQ98Y4w+8DdwMtAXuNca0LaVdBPAYsMFlWVtgENAO6AO8\n49yeRwUF2QDh7Nmznn5qpZSqVtzp6XcBkkRkv4jkAwuA/qW0exF4BXAdR+kPLBCRPBH5FUhybs+j\ngoNtAOTm6hCPUsq3uRP6TYFDLveTncuKGGM6As1FZGlF1/UEm80K/ZwcDX2llG9zJ/RNKcuKBseN\nMX7AG8CTFV3XZRsjjDGbjTGbU1JS3CipYgp7+tnZGvpKKd/mTugnA81d7jcDjrjcjwDaA6uNMb8B\nVwOLnQdzy1sXABGZISLxIhLfoEGDir0CN4SEWKGfmamhr5Tybe6E/iagtTGmlTEmCOvA7OLCB0Xk\ntIjUF5EoEYkC1gP9RGSzs90gY0ywMaYV0BrYWOWvohyFwzsa+kopXxdQXgMRsRtjRgPLAX9gtojs\nNMZMAjaLyOLzrLvTGPMxsAuwA38TkYIqqt1t2tNXSilLuaEPICLLgGUllj1fRtueJe6/BLxUyfqq\nRGiojukrpRT4yBW52tNXSimLT4S+9vSVUsriE6EfHm6F/pkzGvpKKd/mE6GvPX2llLL4ROiHhWlP\nXymlwEdCPyJCQ18ppcBHQr9wTF/n3lFK+TqfCP2wsGBAQ18ppXwi9END/YFADX2llM/zidAPCgKw\n6Xz6Simf51Ohn5enoa+U8m0+EfrBwaA9faWU8pHQL+zp5+dr6CulfJtPhb4O7yilfJ1PhH7h8I72\n9JVSvs4nQt/fHzT0lVLKR0IfwM/PxtmzGvpKKd+moa+UUj7EZ0Lf39+G3a6hr5TybT4T+n5+GvpK\nKeUzoR8QoKGvlFI+FfoFBRr6Sinf5lOhb7fneLsMpZTyKp8J/cBAGw5HLiLi7VKUUsprfCb0AwKs\nb8/Kz8/3ciVKKeU9PhP6QUEhAGRmZnq5EqWU8h63Qt8Y08cYs8cYk2SMGVfK4yONMduNMQnGmLXG\nmLbO5VHGmBzn8gRjzPSqfgHuqls3DoDvv//eWyUopZTXlRv6xhh/4G3gZqAtcG9hqLv4QEQ6iEgs\n8Arwustj+0Qk1vkzsqoKr6iGDbvh51eLL774wlslKKWU17nT0+8CJInIfhHJBxYA/V0biEiGy90w\noNodLbXZAgkLu4lly5bpwVyllM9yJ/SbAodc7ic7lxVjjPmbMWYfVk//MZeHWhljthpjvjPGXHtB\n1V6A4GCw2W7h6NGjJCYmeqsMpZTyKndC35Sy7Jyusoi8LSLRwDPAs87FR4EWItIRGAt8YIypdc4T\nGDPCGLPZGLM5JSXF/eorICgIAgP7AOgQj1LKZ7kT+slAc5f7zYAj52m/ALgdQETyRCTNeXsLsA/4\nU8kVRGSGiMSLSHyDBg3crb1CgoKgoKAxsbGxfPPNNxflOZRSqrpzJ/Q3Aa2NMa2MMUHAIGCxawNj\nTGuXu7cAe53LGzgPBGOMuRRoDeyvisIrKjgY8vLguuuuY926dfol6Uopn1Ru6IuIHRgNLAd2Ax+L\nyE5jzCRjTD9ns9HGmJ3GmASsYZwHnct7ANuMMYnAJ8BIETlZ5a/CDUFBkJ8PPXv2JDc3l40bN3qj\nDKWU8qoAdxqJyDJgWYllz7vcfryM9RYCCy+kwKpSGPrXXnstxhhWr15Njx49vF2WUkp5lM9ckRsc\nDHY71K5dh9jYWFavXu3tkpRSyuN8JvSDgqx/C4d4fvzxR3JyrFk37Xa7FytTSinP8ZnQt1nzrXHi\nBPTv35+8vDymT5/Ohx9+SGhoKH/96185ceKEd4tUSqmLzFS3q1Pj4+Nl8+bNVb7dvXuhfXu4+26Y\nNw969+7Npk2bcDgcREZGcuTIEQYMGMCCBQuq/LmVUupiM8ZsEZH48tr5TE+/dWt45hmYPx9WrYJX\nX32V9PR07HY7q1at4s9//jMHDx70dplKKXVR+UzoA4wfD5dcAtOmwRVXXMGsWbNYtGgRl156KZGR\nkaSnp3u7RKWUuqjcOmWzpggJgT59YNEicDhg6NChRY9p6CulfIFP9fQBrr8eTp2CknOuaegrpXyB\nz4X+dddZ/65aVXx5ZGQkOTk55OXleb4opZTyEJ8L/UsugTZtSg99gNOnT3uhKqWU8gyfC32whnjW\nrIGzZ39fVhj6OsSjlKrJfDL0+/aFrCxY6DIrkIa+UsoX+GzoX3YZ/OMfUHhtmoa+UsoX+NQpm4X8\n/Kxz9h98EF58EaKiYPt2DX2lVM3nk6EPcO+9VuBPmFC4RENfKVXz+eTwDkBgIGzZAj//bM3L06uX\nhr5Squbz2Z4+QK1a1g9AixahQICGvlKqRvPZnn5JDRsaIJJTpy489M+cOcNPP/2kbyBKqWpHQ9+p\nfn2AOqSknKr0Nn777TcGDx5MREQEnTp1IioqiilTplDdpq9WSvkunx7ecdWgAUAkKSmV652np6cT\nHx9PdnY2jz76KF26dGHu3Lk88cQTdOvWjS5dulRpvUopVRka+k5WTz+SkycrF/pvvPEGaWlpbN68\nmU6dOgHQp08fGjZsyOLFizX0lVLVgg7vOBX29Cs6Dn/mzBn27dvHlClTGDBgQFHgA9StW5drr72W\nxYsXs3btWmJjY0lLS6vawpVSqgI09J0Ke/qZme6HflZWFs2bNycmJoaMjAyef/75c9r069eP7du3\n85e//IXExESSkpKqrmillKogDX2nwp5+drb7of/1119z8uRJxo8fzxdffMGVV155Tpt+/foBcOjQ\nIcB6o1BKKW/RMX2n8HDw94/Ebs8lNzcXm81W7jqff/45derU4YUXXiAwMLDUNtHR0XTt2pUzZ86Q\nmJiooa+U8irt6TsZA+Hh7s+pX1BQwNKlS+nbt2+xwN+4EUaMgLQ0+OUXeP11WL16LQsWLAAgOzv7\n4rwApZRyg1s9fWNMH+BNwB+YKSIvl3h8JPA3oADIAkaIyC7nY+OB4c7HHhOR5VVXftWqXTuS06et\n0y8bNWpUapuUlBRee+01CgoKSEtLo3///kWPpabCgAFw+DB8+y2cOAEZGXDJJX706GFd+qs9faWU\nN5Ub+sYYf+Bt4EYgGdhkjFlcGOpOH4jIdGf7fsDrQB9jTFtgENAOuARYaYz5k4gUVPHrqBJ160Zy\n8CA8+eSTREZG8vPPP3P27Fk6d+7MW2+9xY8//sjgwYM5duwYAEFBQfTu3RuwpmgeNgxSUuDtt+F/\n/gcuvdQK/TfegD59wgANfaWUd7nT0+8CJInIfgBjzAKgP1AU+iKS4dI+DCi8BLU/sEBE8oBfjTFJ\nzu2tq4Laq1yrVlezd++dbN++CYfDQbt27QgICGD27Nl899137Nu3jzZt2vDVV19ht9vJzc2llnPy\nnvffhyVLrIB/5BEYPBhCQ+Hdd2H0aLj7biv0jx7V0FdKeY87od8UOORyPxm4qmQjY8zfgLFAEHC9\ny7rrS6zbtJR1RwAjAFq0aOFO3RdFs2Z1CQz8hAMHii//+OOPGTJkCMOHD2fKlCmEhYUVPXb8OOze\nDWPGQPfu8Nhj1vLCidyGDIHnnoOVKwMAG4cPa+grpbzHnQO5ppRl50wmIyJvi0g08AzwbAXXnSEi\n8SIS38A6d9Ir6teH9PTi350LMHDgQDIyMvj3v/9dLPBPnYKYGLjuOsjNhZkzrS9ocRUWBt9/D/Pm\nAYRz+rSGvlLKe9wJ/WSgucv9ZsCR87RfANxeyXW9qvD9prSLZgMCzv1QtHCh9V27s2ZZc/Jfdlnp\n223XDqwLdcPJzNSzd5RS3uNO6G8CWhtjWhljgrAOzC52bWCMae1y9xZgr/P2YmCQMSbYGNMKaA1s\nvPCyLw7rqlzrYKw75s2DNm1g6FBoes6gVXHWcE84mZna01dKeU+5oS8idmA0sBzYDXwsIjuNMZOc\nZ+oAjDbG7DTGJGCN6z/oXHcn8DHWQd+vgL9V1zN3AKKjrX8/+KD8tocOwXffwX33Wef4l6d2bYBw\nsrM19JVS3uPWefoisgxYVmLZ8y63Hz/Pui8BL1W2QE+Ki4OHHoLJk62ee1QUtG4N+fmwbBnMnw8h\nIdYY/dy51jp/+Yt72w4NBQjjzBkNfaWU9+g0DCW8+SZs2ACPPnruY1dcYV1xO368dSrmbbf9/umg\nPMZAYGA4ublujh0ppdRFoKFfQmioFey7d1s9/N27rcDu1cvq/ffta02tEBICU6dWbNtBQeHk5WlP\nXynlPRr6pbDZoGNH6/ZVJa5ImDIFunSBCROs4Z+KbVcP5CqlvEtDv4LatLEuyHJjEs5zhISEc+qU\nnrKplPIenWWzEioT+AChoeE4HNk4HI6qLUgppdykoe9BYWHhgPUVi0op5Q0a+h4UEWGFvs60qZTy\nFg19D6pVy5q3Rw/mKqW8RUPfgyIjrZ5+aqqGvlLKOzT0PahOHSv0T5zQM3iUUt6hoe9B9epZoZ+S\noj19pZR3aOh7UP36OryjlPIuDX0PKgz9tDQNfaWUd2joe1CjRlbonzqloa+U8g4NfQ9q3Ng6ZTM9\nXUNfKeUdGvoe1KhRCGDIyNCzd5RS3qGh70G1ahkKvzJRL8pVSnmDhr4H+fuDMeH8/HMWtWvDxIkg\n8vvj69fvIibmBh577H1ycyu+/UOHDnHy5Mkqq1cpVfNo6HuYn1846elZBAfDCy/ADTfAs8/CU0+t\np3v3a9i3bzVvvfUAISFt8POryy23vEGBy7cKZ2ZmEhsby1NPPYW4vGPk5+dz9dVXM3DgQC+8KqXU\nH4XOp+9hgYHhFBR8TXz8XSQl7WfTpmi+/XYEInfj79+AefM2smrVJ6xZs4aTJxuzbNlY2rULZMOG\nUdSu7c+ECRNITEwkMTGR5ORkfv31VwYOHMgll1zCkSNHOHLkCJs3byY+Pt7bL1UpVQ0Z195idRAf\nHy+bN2/2dhkXTa9eH7Nnz2xCQ/fTsmVL1q5dS25uLk2aNOPbb3+gTZsWRW3z8vLp1KkfO3cuJyio\nCX37XsOSJZ8ybNgw7HY7c+bMoVGjRqSmptKyZUtEhLS0NLp160aLFi2IjY1l1KhRRdvbv38/UVFR\n+PnpBzylahpjzBYRKb+3JyLV6qdTp07iS7Zv3y4PPPCA7Nq1q9TH8/PzZezYTwTuEj+/llK/fitJ\nS0sTh8MhR48elfT0dGnevLkA8sYbb8gzzzwjgABSq1Ytyc7OFhGRWbNmCSCxsbGyZs0aT75EpZQH\nAJvFjYzVnv4fxKZN8MQT8MMPkJAAV15pLc/Nhfvv/5ElS17n++9n0bq1MGPGDJo1a8Z9991Hr15z\nyclpx8aN1xAXdyUnTpzg2LFjrFu3jtjYWAC2bNlCSkoKDRo0IC4uDmOMF1+pUqoy3O3pa+j/gaSn\nQ8uW0KcPfPQRHD0Kt9wCW7dCSAjExcGaNeDnBw6HUKvWZWRn+2PMCSCM5OQtBAQ4iIuLIygoiDff\nfJPPP/+cWbNmFT1HTEwM8+bN46qS3wjvBevWraNu3bq0adPG26UoVe25G/o6uPsHEhkJf/sb/Pe/\n8H//B926wS+/wOLFMH269Sng8suhcWPo3t2Qnf0QsJuGDWsh8g27dtWnYcOGfPLJQk6cSKFfv37M\nnj2b8ePHs27dOubMmUNWVhbjxo3z9kslMzOTPn368PDDD3u7FKVqFnfGgDz542tj+hV14oRIWJgI\niDRtKrJxo7Xc4RB5+GGRXr1E7r9fJDpaZODADHnhhUmSlHREQkJERo8W+ec/RerUEYFMgR8kNnab\nOBy/b3/y5MkCyLZt2zzyenJzc2XYsGEye/bsYsunTZsmgPj5+UlqaqqsXr1aNm3a5JGalPojws0x\nfbeCGOgD7AGSgHGlPD4W2AVsA74BWro8VgAkOH8Wl/dcGvrl+/VX68c1rMvTr59IRIT1G+/dW2TK\nFJFhw6z7P/74e7vU1FSx2WwyZMgQ2bt3r0ydOlUee+wx+fnnn916nhMnTsjMmTPlkUcekb///e+S\nl5d3TpusrCz57LPPZOfOnTIgUyzKAAAWE0lEQVRq1KiiA88DBw6Utm3byv333y9t2rSRhg0bCiBT\np06V2rVry9VXX+3+C1bKx1RZ6AP+wD7gUiAISATalmhzHRDqvD0K+MjlsSx3Cin80dC/OGbNsn7b\nsbEiOTnWsowMkdBQkb/+tXjbYcOGFQUxIIGBgeLv7y+DBg2SOXPmyPPPPy/9+/eXjh07ysCBA+W9\n996TU6dOydChQyUgIEAACQ8PF0A6duwojz/+uLz88suydetWefzxxyUsLKzY9seOHStDhgwRPz8/\n6d69u/j7+wsgc+fOlSZNmkhwcLAAEhoaKna73fM7T6k/gKoM/a7Acpf744Hx52nfEfjB5b6GfjWQ\nni4yeLDIL78UX/7gg9YngKys35elpqbKnDlzZPbs2bJt2zY5fvy4jBkzRurWrVs05NKmTRu56aab\npEWLFs5lQWKMnzzyyGOSmJgoDodDFi1aJC1btpTatWsXBbyfn588+OCD8vXXX8uUKVNk3Lhxcvbs\nWXE4HJKZmSkiIuvWrZNx48ZJbm6uPPzwwwIUbWP37t2e22lK/YFUZejfBcx0uX8/MO087acBz7rc\ntwObgfXA7WWsM8LZZnOLFi0u/t5RRb77zvor6NlTZO5ckaNHz22Tny/y+usil12WJwEB22XWrOyi\nxzIzHdKq1ccCtwp8L127ihQUnLuNX3/9Vf71r3/Jzp07K1TfmjVrpFatWjJv3jwB5IMPPqjoS1TK\nJ1Rl6N9dSui/VUbbwc5wD3ZZdonz30uB34Do8z2f9vQ9y+GwDu62aGH9NYBIy5YigwaJrF4t8tFH\nIh06WMu7dxfp1s26ff311k9UlIifn8jixSJTp1qPzZ9f1TU6JD8/X4KCguTpp5+u2o0rVUO4G/ru\nnLKZDDR3ud8MOFKykTHmBuB/gX4ikle4XESOOP/dD6x2Dv+oasIYGDcOfv0VtmyByZOha1dYsQJ6\n9oR77oGMDPj8c1i7Flatgkcegexs68Kw2Fj48EO47TbrdNKOHWH8eEhMhLNnq6pGQ2BgIO3btych\nIYGkpCR2795dNRtXyseUe3GWMSYA+AXoBRwGNgF/EZGdLm06Ap8AfURkr8vyOsAZEckzxtQH1gH9\nRWRXWc+nF2dVD9nZsGgRNGlihb+/v3vrrV5tzRxaUADNm8PUqdabSK1a1gVk7srNBZut+LLhw4ez\ncOFC/P39cTgc7Nixg6ZNm7q/UaVqsCq7OEtE7MBoYDmwG/hYRHYaYyYZY/o5m70KhAP/NcYkGGMW\nO5dfDmw2xiQC3wIvny/wVfURFgaDB0OvXu4HPlhvEHv2wLx5ULs23HGHdbFYvXrwl79YnybAelN4\n7z146inYuxfefNO6uvjAAXjxRQgPh0GDICnp92137NiR06dPU1BQQH5+PkOHDmX79u2sXLmSd999\nl4yMjAq9xoyMDN5//32yLvAbbQoKCkhOTqa8DpRS1YI7Y0Ce/NEx/ZojL0/kgw9E3nlHZNSowovC\nRK66yjpuACLG/H4sISBAJDLSut2tm3VWUWzs79cjbN++XSIiImTJkiXyzjvvFDvtE5BHHnnknBrO\nnDkjJ0+elJzC81TFOkYgIvLAAw8III0aNZJnnnlG/vvf/0p+fn6xNmU5fvy4XHXVVdKkSRMJCQkR\nQO68885zTinVU0yVp6ATrqnq5vRpa/qI776DBg1g4EC45hp4913o0AHatIFbb4WrrrKOE8ybB0OH\nwhdfQN++1jYcDgd+fn6ICGvWrCE1NZXIyEg+/PBD5s6dy44dOzh16hTvv/8+K1asYN++fQAEBwfT\nt29fkpKSOHDgAH/961957bXXuP/++zl8+DBr1qzBbrfTunVrmjRpwoYNG3jqqaeYMGECgYGBxV6H\niHDHHXfw1Vdfcf/99xMREUFBQQFTp05l8ODBvPTSS7Ro0YLTp0/TuXNnwsLCGDt2LKmpqTRp0oR+\n/foRGhrq6d2vajidcE39IRUUWBPGGWMdCG7dGi65xJpX6HyTf544cYKYmBjOnj1Lbm4uISEh9O7d\nm44dO1K7dm2SkpL49NNPad68OYGBgaxdu5aoqCh27txJaGgo+fn5fPXVV0ycOJHc3FxiYmJYsmQJ\nTZo0IS4ujiuvvBKbzcauXbs4efIkK1as4LXXXmPs2LFFNUycOJEXXngBgGHDhpGbm8uCBQto1qwZ\nBw8eLGpXt25d1q5dy+WXX37R9qPyPTqfvqoR3nnHGu558EER57VbZZo1a5bcfPPNMmfOHDl9+nSZ\n7ex2u8yaNUu2bt163u19+umnct9990n79u0lICBAjDHSqlUrad++vTz00ENSUMoFCfv375cnn3xS\njDECyLPPPiu5ubmyfv16OX78uHzzzTdSp04d6dmzZ5lDSA6HQ3788Uf54osvzjv9xf79+4sNW5V0\n5MgRef311+Wxxx6TiRMnFl38pmomqnLuHU/+aOgrV3a7yHPPWdcCXHmlSEpK1WzX4RA5edKasO6V\nV0TmzbOOQZQlNzdXslwvWy7H6tWrZfz48aXOPfTuu+8KIM8995ysWbNG5syZI2PGjJH+/fvL66+/\nLvfcc0/RcQp/f3+ZNGmSJCYmyr59+4reaLZt2yZBQUFy4403nvPmc/bsWXnuueeKprMovJp53Lhx\nbtev/ng09FWN8uWXIjabSJs2InfcIXLTTSJ9+4pMnCjy6KMi9epZB40zMqxPBePGibz0kkh8vEhc\nnEjnziLh4SKXXWZNPle3rhQdQC78adrUegPYt0/kzJnya8rLEzl4UGTHDpHERJFPPhH59NPy1yso\nKJDrr7++2EHokJAQiY6OLgr6v//977J+/Xq59957i7ULCwuTkSNHyhVXXFE0J9G0adOKtn3mzJmi\nbT/wwANF01YMHjxYbDabfPXVV/L000/L4cOHK/mbUNWVu6GvY/rqD2PlShg9GgICrFM6z5yBHTus\n+02aQFoaxMfD999bxwXsdrj6aqhbF/LyrAPF+/ZZp4Fecw1ccYV1LcG118JPP8Frr1kXn4F1/CA+\nHm680Wo3fz7s2mUdgA4KgtRU69TUgoJz6/z6a+taBbAuamvZ0rqIzVVBQQFJSUns37+f6OhooqOj\n8ff3Z8+ePQQEBBAdHQ1YnbK1a9eSkpLCyZMn+eGHH5g3bx52u53FixfzzjvvsGrVKp544gluueUW\nXn31VZYuXcqsWbMYOnRo0fMdOHCANm3akJdnXTd51VVXsXr1anbv3k12djYtW7akefPmnI+IMG/e\nPH755ReCgoKIibmMn3/eyebNmxg+fDh33HEHubm5PPfcc/j7+/Pyyy/rt7B5kB7IVT7h5Emrn56T\nYwVrWhr8619w333WlcQVvXYrMdG6luDXX603gA0brGBv0ACuu856PrvdutisQwcr0GvXtt5kmje3\nrm3w87O+3nLyZPjHP6w3peefh379rIvOjh2zvv0sOBgyM+Gzz6xvP8vPt66ObtaseE25uVZbsN5o\nUlOTOH16L9263UxWVirPPvsk77//PoX/l9955x1GjRp1zmubO3cuv/zyC9HR0Tz00ENERESQmZkJ\nQFBQEK+88gr9+/dn9+7dfPHFF/Tp04e+ffuybNkyDh8+zOeff86XX34JGKwPHgAGm60RubnHqFOn\nOQUFfmRkHABgzJineOONVwHr92Oznf9gvLowGvrK52zaZPXGH3yw6rZ5+jRs22b1+t25onjFCujd\n+/f7w4dbX3O5cGHxdi1bwmWXWV9vmZMDoaHWm0mHDlbb+fOti+K2b7e+GrNhQ4iIsEIfCr8S07rd\nrBmMGfMLMTEHSU9vQELClXTrBnffXfw58/Otb1o7fRq+/XYyGzas5e6776ZJkyZMnTqVpUuXFrU1\nxh+RApo2vZTDh/cD4O9vo6Dg/wgJeYTu3XNp2nQ32dmN2LixEQcPzgNWAEdo1mw8yclLgWl07Nib\nRo0GsHz5LwQE5FO7dir+/vt5+unRPPnk4FL3oYj1Pc9Llixh3759OBwObr/9dv7xj3/gX5ErBX2M\nhr5SXjJzptWbb9vWuiIZrLBOTLR67P7+1ieA9HRr+Oiee6yvvlyyBPr3t3rDhf8tw8PhgQesTzBp\naTBggBX2R45A/frWp4BPP4WS/2WCgqxPKYXDSunp1ieVhATrfq1a1nN17WpNnQFCv37LOHs2hYUL\nG7B0aQ+MeQGRL4FxwA0EBkbw3HPhPP30uVNknD5tvanUq2e9If33vw4eeGAKubl/B07h728jICAE\nqEV+vg2RPfTvP5rWrZtTrx40adKIQYMGERwczOTJkxk3bhxt2rShffv2ZGVlsXz5cgYOHMj8+fMJ\nCAgoc98fP36chg0buj2sJCIkJCSwatUqCgoKaNSoEa1atWLv3r34+fkRHR3Nvn37iIiI4JZbbiGk\nInOJlGH16tUEBgbSvXv3C96WKw19pf6A3nzTOr7w7LPW9Ql+fuV/wigosC5kO3ECWrSAzp2t4xQh\nIfD449Ybx7//bb0xvPmmdfxj0SJYutQarqpfH7Kyfh/Ceu89a70nnoB166zHGzWytl27tvuvJT8f\nVq3KJC/vBLfd1go/P2vWlx078rj66uFkZ88v1j4kJIrAwEvIyPiRO+64l08+mVe0zl13vcrChf+P\n3r3/j3//ezDTpr1O/fr1ueKKK+jQ4QqaNGnM1KlTGTNmDH369GHSpL/z22/72bZtG4mJiezatYsG\nDRoQExNT9GkhMzOThIQE9u/f79briYiIoFOnTlxxxRVERUWxfPlyHA4H06ZNwxhDcnIy11xzDUuX\nLiUhIYHx48djK/Hu+MMPP9CzZ0/sdjvdunUjIyOD+Ph4/vWvf53TtqI09JXyYd9/b13xfOyYdT84\nGN5/v/iQj8NhHdRu1coawnroITh+3DpwvWhRxSbIq6jUVPjqqxwyMoR582DDhrXYbC8REABnzlxL\ny5bP88ADQRw8aH3qmT1bCAnpR07OtwQFXcrZszsRcRRtLyCgLnb7Sdq3786uXVtwOHIB8PPz57LL\n2tCwYVvOnk3l8OHfio59hISE0K5dO3r16sWAAQMIDw/n0KFD/Pbbb8TExPDDDw6ef34/Awa04sYb\nD7N06Sds3bqV7du3Fx38zszMJDMzk7POKWVDQ0M5c+YMAD179uSFF17g4MGDfPzxxxhj2LBhA+Hh\n4QwbNoyPPvqI8PCG/PjjSrp27cr1119Ps2bNGDlyZKX2qYa+Uj5OBI4etc5catz44ob4hRL5/SDv\nunXWVN1padYnjPR06/6LL/5KbGw78vLswBKgMyEh2+nQIZE9exI5fToK+B8iI5Po3HkjBw+2Y8+e\ntjRoYCMlxTqLa8wY65NRw4bWm93Jk9byhg2t6UG+/hp27oQePaxjK6GhVpvISLjrLuusr2bNHKSn\nH6VLlyZkZx9l/Ph/kJ3diuPHL2XfvmV06nQ1994bxKOPDsdutwMQHt6CoKAw8vNP0ajRClJSOpCX\nZ/1uYAF+fiMRyeLyy69i584fKrUPNfSVUn9YOTlWOIeHF39DWLlyJcYEEBzck9BQa5qOiAirTVKS\nNYR1ww3WUJXdDv/8p3Vs4847YfZs6zshzueyy+Dyy63wj462ThPeswemT7c+/WRnl76en591/KR5\nc+vYTGAg1K2bxLFjBxCpRVhYJxwOP3JyrDeUjh2t4y6XXWa9qXz9tXVQPy7OmnKkMjT0lVLKhYj1\n6SEy0voEdOiQdeD5+HHrfrduVmiD1QM3xgrmQgUF1jTgx45Z80Lt2WMdC2nf3lq3bl2r3dat1tlX\nx49bbxw33ghdulhvDFlZZR8XKTydNyqqcq9PQ18ppXxIlX2JilJKqZpDQ18ppXyIhr5SSvkQDX2l\nlPIhGvpKKeVDNPSVUsqHaOgrpZQP0dBXSikfUu0uzjLGpAAHLmAT9YHUKiqnKmldFVNd64LqW5vW\nVTHVtS6oXG0tRaRBeY2qXehfKGPMZneuSvM0ratiqmtdUH1r07oqprrWBRe3Nh3eUUopH6Khr5RS\nPqQmhv4MbxdQBq2rYqprXVB9a9O6Kqa61gUXsbYaN6avlFKqbDWxp6+UUqoMNSb0jTF9jDF7jDFJ\nxphxXqyjuTHmW2PMbmPMTmPM487lE40xh40xCc6fvl6q7zdjzHZnDZudy+oaY742xux1/lvHwzW1\ncdkvCcaYDGPMGG/sM2PMbGPMCWPMDpdlpe4fY5nq/JvbZoyJ83BdrxpjfnY+9yJjTKRzeZQxJsdl\nv02/WHWdp7Yyf3fGmPHOfbbHGNPbw3V95FLTb8aYBOdyj+2z82SEZ/7OROQP/wP4A/uAS4EgIBFo\n66VamgBxztsRwC9AW2Ai8FQ12Fe/AfVLLHsFGOe8PQ6Y7OXf5TGgpTf2GdADiAN2lLd/gL7Al4AB\nrgY2eLium4AA5+3JLnVFubbz0j4r9Xfn/L+QCAQDrZz/b/09VVeJx18Dnvf0PjtPRnjk76ym9PS7\nAEkisl9E8oEFQH9vFCIiR0XkJ+ftTGA30NQbtVRAf+A/ztv/AW73Yi29gH0iciEX6FWaiKwBTpZY\nXNb+6Q+8J5b1QKQxpomn6hKRFSJid95dDzS7GM9dnjL2WVn6AwtEJE9EfgWSsP7/erQuY4wBBgIf\nXoznPp/zZIRH/s5qSug3BQ653E+mGgStMSYK6AhscC4a7fx4NtvTQyguBFhhjNlijBnhXNZIRI6C\n9QcJNPRSbQCDKP4fsTrss7L2T3X6uxuG1Rss1MoYs9UY850x5lov1VTa76667LNrgeMistdlmcf3\nWYmM8MjfWU0JfVPKMq+elmSMCQcWAmNEJAP4FxANxAJHsT5aekN3EYkDbgb+Zozp4aU6zmGMCQL6\nAf91Lqou+6ws1eLvzhjzv4AdmO9cdBRoISIdgbHAB8aYWh4uq6zfXbXYZ8C9FO9ceHyflZIRZTYt\nZVml91lNCf1koLnL/WbAES/VgjEmEOuXOV9EPgUQkeMiUiAiDuDfXKSPtOURkSPOf08Ai5x1HC/8\nuOj894Q3asN6I/pJRI47a6wW+4yy94/X/+6MMQ8CtwL3iXMA2Dl0kua8vQVr3PxPnqzrPL+76rDP\nAoABwEeFyzy9z0rLCDz0d1ZTQn8T0NoY08rZWxwELPZGIc6xwlnAbhF53WW56xjcHcCOkut6oLYw\nY0xE4W2sA4E7sPbVg85mDwKfe7o2p2K9r+qwz5zK2j+LgQecZ1dcDZwu/HjuCcaYPsAzQD8ROeOy\nvIExxt95+1KgNbDfU3U5n7es391iYJAxJtgY08pZ20ZP1gbcAPwsIsmFCzy5z8rKCDz1d+aJo9We\n+ME6wv0L1jv0/3qxjmuwPnptAxKcP32B94HtzuWLgSZeqO1SrDMnEoGdhfsJqAd8A+x1/lvXC7WF\nAmlAbZdlHt9nWG86R4GzWD2s4WXtH6yP3W87/+a2A/EerisJa6y38O9surPtnc7fbyLwE3CbF/ZZ\nmb874H+d+2wPcLMn63IunwuMLNHWY/vsPBnhkb8zvSJXKaV8SE0Z3lFKKeUGDX2llPIhGvpKKeVD\nNPSVUsqHaOgrpZQP0dBXSikfoqGvlFI+RENfKaV8yP8HeWLA+3iKcXAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6070b6be48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "thetRand,featIndRand,threshsRand,sgnsRand = randomBooster(featuresTrain,labelsTrain,200)\n",
    "plotErrorOverTime(featuresTrain,labelsTrain,featuresTest,labelsTest,thetRand,featIndRand,threshsRand,sgnsRand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, optimal boosting fairly quickly (~25 iterations) converges to a training error below where the test eror bottoms out at around 22%. Random boosting converges towards the same performance, but much slower. Even after 200 iterations it has not converged to the minimum error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
